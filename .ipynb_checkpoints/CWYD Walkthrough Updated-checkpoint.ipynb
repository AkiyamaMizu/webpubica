{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fAhsUTqCDeGq"
   },
   "source": [
    "<img src=\"https://about.nyp.ai/static/logo/Dark.png\" alt=\"NYP AI Logo\" height=\"100px\">\n",
    "\n",
    "# **Introduction to LangChain: Chat With Your Data**\n",
    "\n",
    "Updated: 10 July 2024, 10:33PM\n",
    "\n",
    "**Welcome to NYP AI's Chat With Your Data Workshop.**\n",
    "\n",
    "**What?**\n",
    "\n",
    "In this workshop, code along with the instructor and build your own data inference algorithm with the [LangChain Python library](https://langchain.com).\n",
    "\n",
    "**HELP MEE**\n",
    "\n",
    "For Non-Technical or Curious Questions:\n",
    "\n",
    "> Ask them [here](https://qna.nyp.ai/ask) and the instructor will try to answer them in inter-segment breaks or at the Q&A session at the end.\n",
    "\n",
    "For Technical Questions or Having Trouble Following Along:\n",
    "> Feel free to raise your hand at any time and one of the workshop troubleshooters will assist you.\n",
    "\n",
    "***Note: Please be polite and co-operative, we want to ensure you have a good learning experience and we hope that you will allow us to create that.***\n",
    "\n",
    "**How do I start?**\n",
    "\n",
    "For instructions on setting up this notebook, look at the [CWYD Workshop Pre-Requisites](https://docs.google.com/document/d/e/2PACX-1vRwOmZCrFxrWwbTamFt9eBxprybm4_xNUaSUofVW3Iys50IM15i9yF9oqjmWd32GuG6ZCqYMIo3XVFl/pub) document.\n",
    "\n",
    "---\n",
    "\n",
    "We hope you have takeaway valuable skills from today and that you had fun! 🤩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDk6MIQDHzf4"
   },
   "source": [
    "# 0. Setup\n",
    "Let's get started!\n",
    "Start by installing the required libraries and getting your workshop account API key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvu8lP0JH8kG"
   },
   "source": [
    "### 0.1 Install required libraries\n",
    "The following libraries and imports are required for this notebook.\n",
    "\n",
    "> **Run** the following cell by clicking on the cell and doing **'Shift + Enter'** or clicking the run button at the top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ci8E90M6JzSt",
    "outputId": "af17c9f0-ecb3-4545-9dd4-e2d5c85f4da9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install langchain openai langchain-community langchain-openai chromadb lark\n",
    "import os, sys, json, shutil\n",
    "from pprint import pprint\n",
    "from langchain.document_loaders import NotionDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "print()\n",
    "print(\"All libraries and imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbnVMBSFLo8O"
   },
   "source": [
    "### 0.2 Get the workshop API Key\n",
    "\n",
    "This API key will be used by LangChain to send structured queries to OpenAI endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OPEce6J6LuL0"
   },
   "outputs": [],
   "source": [
    "# Run this cell first\n",
    "exec(\"\"\"\\nimport os, sys, json, requests\\n\\ndef injectAPIKey(username,password,injectionKey=\"OPENAI_API_KEY\"):\\n    hd = {\"Content-Type\":\"application/json\",\"APIKey\":\"P@ssw0rd!\"}\\n    d = requests.post(url=\"https://keyserver.replit.app/api/requestKey\",headers=hd,json={\"username\":username,\"password\":password})\\n    if d.text.startswith(\"UERROR\") or d.text.startswith(\"ERROR\"):\\n        raise Exception(\"INJECTAPIKEY ERROR: \" + d.text[len(\"ERROR: \"):])\\n    elif d.text.startswith(\"SUCCESS\"):\\n        os.environ[injectionKey] = d.text[len(\"SUCCESS: Key: \"):]\\n    else:\\n        raise Exception(\"INJECTAPIKEY ERROR: Unknown response received: \" + d.text)\\n\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ng2rkwWoljh4"
   },
   "source": [
    "**Where to get the USERNAME and PASSWORD?**\n",
    "\n",
    "Details should've been sent to your email. Pass it into the `injectAPIKey` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ac1bgcOaJGWu"
   },
   "outputs": [],
   "source": [
    "# Uncomment the line of code below and replace parameters with your username and password\n",
    "# injectAPIKey(\"USERNAME HERE\", \"PASSWORD HERE\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3hWK-pVJINl"
   },
   "source": [
    "---\n",
    "\n",
    "# Segment 1 - Loading, Splitting, Embedding\n",
    "Now that you have finished setting up, let's get right into it!\n",
    "\n",
    "Here, you'll be learning the theory of Retrieval Augmented Generation and the different stages in the process.\n",
    "\n",
    "In the hour, we will tough on Document Loaders, Splitters and Embedding splits into a Vector Database; all part of the **Indexing Pipline** shown below.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*vAvDBIbr8MnL_Q51mBtBhw.png\" alt=\"Indexing Pipeline\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xydUKvSpJLoS"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "**What are LLMs?**\n",
    "\n",
    "Large Language Models (LLMs) demonstrate significant capabilities in understanding and generating human language. They can perform various tasks such as answering questions, summarizing text, and generating creative content.\n",
    "\n",
    "However, they **sometimes generate incorrect but believable responses** when they lack information, a **phenomenon known as “hallucination.”** This means they **confidently provide information that may sound accurate but could be incorrect due to outdated or insufficient knowledge**.\n",
    "\n",
    "> In the context of this workshop, LLMs are powerful tools, but they need proper mechanisms to ensure the accuracy and relevance of their responses.\n",
    "\n",
    "## Where RAG comes in...\n",
    "\n",
    "**What is RAG?**\n",
    "\n",
    "Retrieval Augmented Generation (RAG) **addresses the issue of LLM hallucinations** by integrating an information retrieval system into the LLM pipeline. Instead of relying solely on pre-trained knowledge, RAG allows the model to dynamically **fetch information from external knowledge sources when generating responses**. This dynamic retrieval mechanism ensures that the information provided by the LLM is not only **contextually relevant** but also **accurate and up-to-date**.\n",
    "\n",
    "> In summary, RAG enhances the reliability of the conversation by grounding responses in real-time data, making interactions more trustworthy and informative.\n",
    "\n",
    "***Below is a simplified RAG pipeline:***\n",
    "\n",
    "<img src=\"https://assets-global.website-files.com/5ee50f2ef83ac07f0cb7fb44/65847f3073978e597886d087_rag-f517f1f834bdbb94a87765e0edd40ff2.png\" alt=\"RAG Pipeline\" height=\"400px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGg21djaKdXA"
   },
   "source": [
    "## 1.1 Document Loading\n",
    "\n",
    "**What is Document Loading?**\n",
    "\n",
    "The first step is to transform your data into a format conducive for interaction. We do this by embedding your your source data into a semantic numerical format for retrieval, as you'll learn soon.\n",
    "\n",
    "**Why is it Important?**\n",
    "\n",
    "Document loaders play a crucial role in ***accessing and converting data from a multitude of formats and sources into a standardized structure.***\n",
    "\n",
    "We often find ourselves needing to extract data from various origins such as websites, databases, YouTube, and this data can manifest in diverse formats like PDFs, HTML, and JSON. The primary objective of document loaders is to harmonize this data diversity into a unified document object, comprising content and associated metadata.\n",
    "\n",
    "**Where Langchain Comes In...**\n",
    "\n",
    "In LangChain, you'll discover an extensive range of ***document loaders***, roughly categorized into more than 80 distinct types. These loaders cater to unstructured data, such as text files from public sources like YouTube, Twitter, or Hacker News, as well as unstructured data from proprietary sources like Figma or Notion.\n",
    "\n",
    "> Document loaders also extend their capabilities to structured data, often presented in tabular formats, containing text data within cells or rows that are still essential for question answering or semantic search.\n",
    "\n",
    "For this workshop, we will be using a Notion Database of Harry Potter information.\n",
    "\n",
    "> **How to Load Notion Databases (IN GENERAL):**\n",
    ">\n",
    "> 1. Export your Notion space as Markdown/CSV\n",
    "> 2. Enable 'Include subpages' and 'Create folders for subspages'\n",
    "> 3. Unzip the folder and place it in the same folder as this .ipynb file\n",
    "> 4. Use Langchain's Document Loader to load your Notion DB with steps similar to what's shown.\n",
    "\n",
    "Follow the instructions below to setup the Harry Potter Notion DB and load it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WhkC5DclJzUB"
   },
   "source": [
    "### Prepare Harry Potter Notion DB\n",
    "\n",
    "> In the same folder as this notebook, [download and unzip the HogwartsDB Notion dump](https://go.nyp.ai/cwydhogwarts).\n",
    "\n",
    "The unzipped folder should directly have 6 files of Harry Potter text; there should be no sub-folders. Some operating systems may auto-create subfolders with the same name in the unzipped folder, so you need to move the files up one folder.\n",
    "\n",
    "Resulting folder structure should look like:\n",
    "```\n",
    "- CWYD Walkthrough.ipynb\n",
    "- HogwartsDB\n",
    "    - Harry Potter and The Sorcerer's Stone.md\n",
    "    - Harry Potter and the Chamber of Secrets.md\n",
    "    - Harry Potter and the Prisoner of Azkaban.md\n",
    "    - Harry Potter and the Goblet of Fire.md\n",
    "    - Harry Potter and the Order of the Phoenix.md\n",
    "    - Harry Potter and the Half-Blood Prince.md\n",
    "```\n",
    "\n",
    "*If you face issues with unzipping and loading the HogwartsDB in the subsequent steps, seek help from a troubleshooter.*\n",
    "\n",
    "> You need to take note of where you're storing this notebook file. If you don't remember, run the cell below to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P7dHRYl4_uBH"
   },
   "outputs": [],
   "source": [
    "# (OPTIONAL) RUN TO CHECK CURRENT FOLDER AND TO SEE IF HOGWARTSDB FOLDER IS FOUND\n",
    "print(\"Notebook's Current Folder Path:\", os.getcwd())\n",
    "print(\"HogwartsDB folder found in current folder:\", os.path.isdir(os.path.join(os.getcwd(), \"HogwartsDB\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bEZeHeYa_uBH"
   },
   "source": [
    "### Loading HogwartsDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c2OT5kMKJ3yz"
   },
   "outputs": [],
   "source": [
    "# Initialise a NotionDirectoryLoader and load the database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qMIVqHM5NJOU"
   },
   "outputs": [],
   "source": [
    "# Check if data has loaded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ma3i2r5PO-Oh"
   },
   "outputs": [],
   "source": [
    "# see the metadata of the database\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "neU74-QaWy_R"
   },
   "source": [
    "## 1.2 Text / Document Splitting\n",
    "\n",
    "**What is Document Splitting?**\n",
    "\n",
    "Document splitting is a pre-processing step that ***divides large textual documents into smaller segments or chunks***. This technique is essential for ***managing and processing large volumes of text efficiently***, especially in natural language processing (NLP) tasks.\n",
    "\n",
    "Once documents are split, each segment or chunk becomes more manageable for further analysis and processing. This segmentation allows NLP models to handle pieces of text individually, improving computational efficiency and enabling more targeted analysis.\n",
    "\n",
    "**Chunking Method using Fixed Chunk Sizes & Overlapping**\n",
    "\n",
    "One method to document splitting is by...\n",
    "\n",
    "**Chunk Size**\n",
    "\n",
    "The size of the chunked data is going to make a huge difference in what information comes up in a search. When you embed a piece of data, the whole thing is converted into a vector. Include too much in a chunk and the vector loses the ability to be specific to anything it discusses. Include too little and you lose the context of the data.\n",
    "\n",
    "> In short, size matters. 😁\n",
    "\n",
    "**Chunk Overlapping**\n",
    "\n",
    "For some LangChain splitters, you can specify a specific chunk overlap; chunk overlaps help to precede chunks with information from the previous chunk so that the chunk split is not too abrupt. The specified quantity of overlap is included in both the end and the beginning of the first and second chunks respectively.\n",
    "\n",
    "This helps chunks be more useful and not too abrupt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZfQa_j7nXC-k"
   },
   "source": [
    "### Common LangChain Text Splitters\n",
    "\n",
    "LangChain provides an extensive range of different text splitters. Some common ones include:\n",
    "- Character Text Splitter\n",
    "- Token Text Splitter\n",
    "- Recursive Character Text Splitter\n",
    "- Markdown Header Text Splitter (also known as 'Context-aware chunking')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xc0f8Bn7_uBI"
   },
   "source": [
    "### Understanding LangChain Text Splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jaKR-fDZ_uBI"
   },
   "outputs": [],
   "source": [
    "# Initialise a CharacterTextSplitter and RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ep99tN1d_uBI"
   },
   "outputs": [],
   "source": [
    "# Split text1 with r_splitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NtPPO_D7_uBI"
   },
   "outputs": [],
   "source": [
    "# Split text2 with r_splitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kBBVk0bH_uBI"
   },
   "outputs": [],
   "source": [
    "# Split text3 with r_splitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uJA28bHN_uBI"
   },
   "outputs": [],
   "source": [
    "# Split text3 with c_splitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fIkyFCQs_uBI"
   },
   "outputs": [],
   "source": [
    "# Split text3 with new c_splitter with space separator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7QT1sUsSJAj"
   },
   "source": [
    "### Splitting HogwartsDB (RecursiveSplitter)\n",
    "\n",
    "For this workshop, we will be using `RecursiveCharacterTextSplitter` to split our data. As you'll learn, the splitter splits based on a list of separators, ordered by priority in terms of highest to lowest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LV0aSISjW3fO"
   },
   "outputs": [],
   "source": [
    "# Define reasonable chunk parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5U-E7QAA_uBI"
   },
   "outputs": [],
   "source": [
    "# Initialise new RecursiveCharacterTextSplitter and split with split_documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vr9IpGVb_uBI"
   },
   "source": [
    "## 1.3 Embedding Chunks into a Vector Store / Database\n",
    "\n",
    "**What are Embeddings?**\n",
    "\n",
    "Embeddings are numerical representations of real-world objects that machine learning (ML) and AI systems use to understand complex knowledge domains like humans do.\n",
    "\n",
    "As an example, computing algorithms understand that the difference between 2 and 3 is 1, indicating a close relationship between 2 and 3 as compared to 2 and 100.\n",
    "\n",
    "We will be using the `OpenAIEmbeddings` module, which uses embedding models made by OpenAI.\n",
    "\n",
    "**What are Vectorstores?**\n",
    "\n",
    "A vector store is an actual system or platform to handle the complexities and specifics of vector data, like embeddings, often in association with a vector database. They are very commonly used in AI and ML applications.\n",
    "\n",
    "Popular examples of vector databases include Pinecone, Chroma and many more.\n",
    "\n",
    "**Importance of Vectorstores for Embeddings**\n",
    "\n",
    "By storing embeddings in a vector store, we can perform really efficient searches and retrievals, allowing us to retrieve the most relevant documents or chunks of text for a given query.\n",
    "\n",
    "There are many vectorstores that you can use to store your embeddings. For this workshop, we will be using ChromaDB to store our Hogwarts Database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTy1mirK_uBI"
   },
   "source": [
    "### Embedding our DB Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HrvNwQhT_uBJ"
   },
   "outputs": [],
   "source": [
    "# Initialise OpenAIEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FNv0fXTI_uBJ"
   },
   "outputs": [],
   "source": [
    "# Initialise a Chroma vector database. Persist in a './db/chroma' folder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check your vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w-pEy368_uBJ"
   },
   "outputs": [],
   "source": [
    "# Check vectorDB collection count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLrN1P2t_uBJ"
   },
   "source": [
    "## 1.4 Segment 1 Checkpoint\n",
    "\n",
    "Wow, that was intense! Let's **summarise** what we have learnt so far:\n",
    "1. **Document Loading**\n",
    "  - Using a few of LangChain's loaders to load a Notion Dump\n",
    "\n",
    "2. **Document Splitting**\n",
    "  - Using LangChain's different splitters to split different kinds of data in different ways\n",
    "\n",
    "3. **Embeddings**\n",
    "  - Using the `OpenAIEmbeddings` module (and the `Text-embedding-ada-002-v2` model) to embed splits\n",
    "\n",
    "4. **Vector Storing**\n",
    "  - Storing embeddings into a local vector `Chroma` database\n",
    "\n",
    "\n",
    "While we have only went through the basics, we do encourge you guys to **stay curious** and explore more on the different methods for each step!\n",
    "\n",
    "Explore:\n",
    "- [All the different LangChain loaders available](https://python.langchain.com/v0.2/docs/integrations/document_loaders/)\n",
    "- [Explore different data splitters and parameters](https://python.langchain.com/v0.2/docs/integrations/document_transformers/)\n",
    "- [Learn about how embedding models work](https://medium.com/@eugenesh4work/what-are-embeddings-and-how-do-it-work-b35af573b59e)\n",
    "- [Learn about different Vector stores](https://python.langchain.com/v0.2/docs/integrations/vectorstores/)\n",
    "- [Learn more about Chroma](https://www.trychroma.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dbOU9qH_uBJ"
   },
   "source": [
    "---\n",
    "# Segment 2 - Retrieval Algorithms\n",
    "\n",
    "**What is Retrieval?**\n",
    "\n",
    "After storing our embeddings into a vector store, we can must now look at how we can retrieve the appropriate splits that is relevant to our Prompt / Search Query to load into the LLM.\n",
    "\n",
    "**Importance of Retrieval Algorithms**\n",
    "\n",
    "Retrieval algorithms are then important since they are the core techniques for the retrieval of data in response to a user's query. They are responsible for retrieving information that is potentially useful for the LLM to answer the user appropriately.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0KfLMjq_uBK"
   },
   "source": [
    "## 2.1 Common Retrieval Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8TONHnH_uBK"
   },
   "source": [
    "### Semantic Similarity Search\n",
    "\n",
    "**How it works?**\n",
    "\n",
    "Taking advantage of a vector database's properties, this technique allows you to retrieve the most similar document chunks for a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pm-ed18k_uBK"
   },
   "outputs": [],
   "source": [
    "# Carry out a basic semantic similarity search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaV0UdSc_uBK"
   },
   "source": [
    "### Filtered Similarity Search\n",
    "\n",
    "Building on the basic semantic similarity search, we can add in a filter to it.\n",
    "This `filter` parameter limits the search to ONLY retrieve from the splits inside the stated document souce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_T3Vce0Y_uBK"
   },
   "outputs": [],
   "source": [
    "# Filtered Similarity Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMn5abFG_uBK"
   },
   "source": [
    "### MMR Search (Diverse retrieval)\n",
    "\n",
    "**How it works?**\n",
    "\n",
    "The idea behind Maximum Marginal Relevance (MMR) is to reduce redundancy and increase diversity in the results. MMR selects the phrase in the final keyphrases list according to a combined criterion of query relevance and novelty of information.\n",
    "\n",
    "In LangChain, you provide a initial `fetch_k` to indicate the number of similar chunks you want to retrieve. From this, the specified `k` **diverse** chunks will be returned as the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cbX_dvMi_uBK"
   },
   "outputs": [],
   "source": [
    "# Maximum Marginal Relevance Search (Diverse retrieval)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZX9WcKa_uBK"
   },
   "source": [
    "## 2.2 BONUS: Self-query Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XAuB0q5H_uBL"
   },
   "source": [
    "Often, you want to infer from the metadata itself.\n",
    "\n",
    "To address this, we can use `SelfQueryRetriever`, which uses an LLM to extract:\n",
    "1. The `query` string to use for vector search\n",
    "2. A metadata filter to pass in\n",
    "\n",
    "Most vector databases support metadata filters, so this doesn't require any new databases or indexes.\n",
    "\n",
    "[Try out self-query retrieval by referring to this.](https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/self_query/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWLbBcP9_uBL"
   },
   "source": [
    "## 2.3 Segment 2 Checkpoint\n",
    "\n",
    "This is a good point to stop and explore for a second. Take a review of all that you've learned in this section. Try out different kinds of queries and see the outputs you get. Play around with the parameters you pass in and see what parameters work the best.\n",
    "\n",
    "\n",
    "You can also explore other kinds of search like `asimilarity_search`, `similarity_search_with_score` and many more. Try passing different parameters to the retrieval chains and experimenting with different prompts.\n",
    "\n",
    "[Learn more about the large variety and complexities of LangChain retrievers here](https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PDomJTTm_uBL"
   },
   "source": [
    "---\n",
    "# Segment 3 - Question Answering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOS5QdP9_uBL"
   },
   "source": [
    "## 3.0 Setting Up LangSmith (OPTIONAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8l7x9aE_uBL"
   },
   "source": [
    "This is entirely optional; the instructor will show you the LangSmith console during the workshop to explain what's going on.\n",
    "\n",
    "The benefit of linking up to the LangSmith platform is the ability to visualise the LLM calls and different steps a chain takes.\n",
    "\n",
    "If you want to link up with LangSmith, carry out the following:\n",
    "- Go to [LangSmith](https://www.langchain.com/langsmith) and sign up\n",
    "- Create an API key from your account settings\n",
    "- Uncomment the code below and use your API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cTYuS0sj_uBL"
   },
   "outputs": [],
   "source": [
    "# Set up LangSmith\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.langchain.plus\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = \"...\" # replace dots with your API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L2kh7lBl7nE7"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "**What is Retrieval Questions Answering Chains?**\n",
    "\n",
    "Retrieval QA chains are designed for question-answeing tasks where the answer is retrieved from a given context. Chains are highly modular; you can combine them with other chains, re-order them and even introduce your own steps in between.\n",
    "\n",
    "**Importance of 'Chains'**\n",
    "\n",
    "Retrieval chains play an important role in the retrieval process, providing a streamlined process of flow and maintaining the efficiency and relevancy of information extracted from external sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O47i0s2x_uBL"
   },
   "source": [
    "## 3.1 Stuff QA Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GAU7IvZY_uBL"
   },
   "source": [
    "### Making Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8FjpCdrD_uBL"
   },
   "outputs": [],
   "source": [
    "# Initialise a PromptTemplate with a given string template\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0) # try experimenting temperature with values from 0-1\n",
    "\n",
    "# Build your prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYV13umD_uBL"
   },
   "source": [
    "### Running a QA Chain (Stuff Technique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2kCMgDz9_uBL"
   },
   "outputs": [],
   "source": [
    "# Initialise a RetrievalQA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LMb4V2G8_uBM"
   },
   "outputs": [],
   "source": [
    "# Run user query through the chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W-SYJrPi_uBM"
   },
   "outputs": [],
   "source": [
    "# See the result's source documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6XAm0IP_uBM"
   },
   "source": [
    "The stuff technique is really good because it involves only one call to the language model.\n",
    "\n",
    "The problem with this is that if there's too many documents, they may not all be able to fit in the LLM's context window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GUEkKyVe_uBM"
   },
   "source": [
    "## 3.2 MapReduce QA Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRe9Yx5s_uBM"
   },
   "source": [
    "In the Map Reduce technique, each retrieved chunk is passed into individual LLM calls to be summarised.\n",
    "\n",
    "These summarised chunks are then stuffed into one final LLM call with the user's prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TuusGKF6_uBM"
   },
   "source": [
    "### Create a MapReduce chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A57qsRrF_uBM"
   },
   "outputs": [],
   "source": [
    "# Initialise a MapReduce RetrievalQA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1R3cK3OU_uBM"
   },
   "outputs": [],
   "source": [
    "# Run user query through the MapReduce chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDCB2-23_uBM"
   },
   "source": [
    "> Note how the map reduce chain took **significantly longer**? In some cases, map reduce even **performs worse than a stuff technique**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-ZyhbTf_uBM"
   },
   "source": [
    "### Why is it taking longer?\n",
    "\n",
    "This is due to a few reasons that you can uncover by looking at the run trace in LangSmith:\n",
    "- MapReduce summarises each retrieved chunk in separate LLM calls first\n",
    "- These summarised chunks are then stuffed into a regular `StuffDocumentsChain` with a call to the LLM with the initial user query.\n",
    "- However, **these summarised chunks may not be an accurate representation or may have missing information from the original chunk**, explaining the longer wait times and the inaccuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_TMAn7O_uBM"
   },
   "source": [
    "## 3.3 Refine QA Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dIjVbWY0_uBM"
   },
   "source": [
    "In a chain using the refine technique, LangChain will invoke sequential calls to the LLM.\n",
    "\n",
    "In each call, LangChain provides a chunk or more of context to the LLM and prompts with the user question. In subsequent calls, the previous response is **combined with new data/chunks and the LLM is prompted to refine it's original answer**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Bt-6-B4_uBM"
   },
   "source": [
    "### Create a Refine chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KbspemzD_uBM"
   },
   "outputs": [],
   "source": [
    "# Initialise a Refine RetrievalQA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZCyxjGZH_uBN"
   },
   "outputs": [],
   "source": [
    "# Run user query through the Refine chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxqcwLPl_uBN"
   },
   "source": [
    "As you can see, through iterative refinements, the LLM's output is much more well-phrased and comprehensive.\n",
    "\n",
    "The output is also better than when you ran the map reduce chain, because the refine chain actually emphasises more carrying over of information than the former chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5KRMOStX_uBN"
   },
   "source": [
    "But, you still can't ask follow up questions. The whole point of a chatbot is to be able to have follow-up questions right?\n",
    "\n",
    "**Let's fix that.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7bv3gK6v_uBN"
   },
   "source": [
    "## 3.4 Conversational Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TM1SoaOO_uBN"
   },
   "source": [
    "INTRODUCE A BIT OF THEORY ABOUT CONVERSATIONS, AND HOW THEY USE MEMORY BUFFERS. INTRODUCE LANGCHAIN'S CONVERSATIONALRETRIEVALCHAIN.\n",
    "\n",
    "**What are RAG Conversations?**\n",
    "\n",
    "Thanks to the modular architecture of chains, you can nest chains within each other and pass data to and fro.\n",
    "\n",
    "As a result, you can then create conversational chains, so that, while simultaneously retrieving the most relevant information, information about the conversation history is also included to make the answer well-informed.\n",
    "\n",
    "**What are Memory Buffers?**\n",
    "\n",
    "Memory buffers in LangChain allow for the storing of messages which are later formatted into input variables for the prompt. This tool allows you to quickly maintain conversation state and create powerful conversational chains.\n",
    "\n",
    "**What is a `ConversationalRetrievalChain`?**\n",
    "\n",
    "`ConversationalRetrievalChain` is a module from LangChain which allows you to quick create a conversational interface with your data, provided a memory buffer and vector database to retrieve from. This is how we will create a conversational chat interface to talk to our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVr26Q8e_uBN"
   },
   "source": [
    "### Create a Memory Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tVPWSVpa_uBN"
   },
   "outputs": [],
   "source": [
    "# Initialise a ConversationBufferMemory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XclV0mSd_uBN"
   },
   "source": [
    "### Create a ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mSuK82uZ_uBN"
   },
   "outputs": [],
   "source": [
    "# Initialise a ConversationalRetrievalChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rNsBERHU_uBN"
   },
   "outputs": [],
   "source": [
    "# Run sequential user queries through the chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FwrlhUq9_uBN"
   },
   "source": [
    "***And that's it!***\n",
    "\n",
    "**Congratulations! You can now *Chat With Your Data!*** 🤯🎉🥳"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNSEXTxD_uBN"
   },
   "source": [
    "## 3.5 Segment 3 Checkpoint\n",
    "\n",
    "Excitingly, now you have finally created a working chat interface with your own custom data.\n",
    "\n",
    "Now that you've written the algorithm, hopefully you can see how it all falls into place together to **create a streamlined Retrieval Augmented Generation workflow.**\n",
    "\n",
    "This workflow algorithm is highly modular, you can substitute, modify, add, remove any components or logic however you want, as long as the core concepts and procedures of RAG are there. You can introduce your own custom logic as well for more niche use cases.\n",
    "\n",
    "**As a recap of this segment, you:**\n",
    "- Created a stuff `RetrievalQA`, where you discovered that it may not be ideal for cases where the documents overflow the LLM's context window\n",
    "- Created a map reduce `RetrievalQA`, which summarises chunks (\"reduces\") and then collates them into one final LLM call. But, map reduce is often inaccurate\n",
    "- Created a refine `RetrievalQA`, which incrementally refines an LLM's outputs by combining new data/chunks with previous answers to the prompt\n",
    "\n",
    "\n",
    "**Cheatsheet:**\n",
    "- Fastest - Stuff QA\n",
    "- Slowest - MapReduce QA, Refine QA\n",
    "- Most Accurate & Comprehensive - Refine QA\n",
    "- Least LLM calls - Stuff QA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKFF1BEG_uBN"
   },
   "source": [
    "---\n",
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47twLLj1_uBO"
   },
   "source": [
    "**Give yourself a pat on the back for successfully following through this workshop and creating your own data-inferring chatbot! This is no small feat! 🎉**\n",
    "\n",
    "\n",
    "**Let's recap all that you have learnt:**\n",
    "- Loading structured and unstructured data with LangChain loaders, especially `NotionDirectoryLoader`\n",
    "- Splitting data in different ways with `RecursiveCharacterTextSplitter` and `CharacterTextSplitter`\n",
    "- Embedding data with `OpenAIEmbeddings` in a local `chroma` vector database\n",
    "- Implementing basic retrieval algorithms like `similarity_search` and `max_marginal_relevance`\n",
    "- Answering questions with chains using `stuff`, `map_reduce` and `refine` techniques\n",
    "- Creating a `ConversationalRetrievalChain` where you can ask follow-up prompts\n",
    "\n",
    "\n",
    "**So, What's next?**\n",
    "\n",
    "You've just learnt the basics of Retrieval Augmented Generation with LangChain in Python. You are now fully equipped to integrate these RAG algorithms into your own personal/school projects for an amazing new AI-powered touchpoint with your users.\n",
    "\n",
    "Additionally, empowered by the basic knowledge, you can go on to further research RAG and all the complex upgrades you introduce in your own algorithms. The world is full of possibilities; **go crazy!**\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "### **CREDITS**\n",
    "\n",
    "**Proudly delivered by...**\n",
    "\n",
    "> This workshop was a massive undertaking in the works for **more than three months** of efforts various people in the **NYP AI Student Interest Group**.\n",
    "\n",
    "\n",
    "At NYP AI internally, we aimed to train ourselves in RAG and developed our own project with team members consisting of:\n",
    "- [Prakhar Nilesh Trivedi](https://linkedin.com/in/prakhartrivedi0706)\n",
    "- [Sarah Zoe Sung](https://www.linkedin.com/in/sarah-zoe-sung/)\n",
    "- [Derron Foo Xi Wei](https://www.linkedin.com/in/derron-foo-xi-wei-a90896298/)\n",
    "- [Peh Jun Jie Rone](https://www.linkedin.com/in/ronepeh/)\n",
    "- [Gabriel Lim Wen Le](https://www.linkedin.com/in/gabriel-lim-wen-le-3b26612b0/)\n",
    "- [Hoi Sing See](https://www.linkedin.com/in/hoi-sing-see-/)\n",
    "\n",
    "\n",
    "NYP AI's Chat With Your Data Workshop has been proudly delivered to you by the event committee, consisting of:\n",
    "- [Prakhar Nilesh Trivedi](https://linkedin.com/in/prakhartrivedi0706) (OIC, VP)\n",
    "- [Sarah Zoe Sung](https://www.linkedin.com/in/sarah-zoe-sung/) (AIC)\n",
    "- [Derron Foo Xi Wei](https://www.linkedin.com/in/derron-foo-xi-wei-a90896298/) (Materials and Content)\n",
    "- [Peh Jun Jie Rone](https://www.linkedin.com/in/ronepeh/) (Materials and Content)\n",
    "- [Gabriel Lim Wen Le](https://www.linkedin.com/in/gabriel-lim-wen-le-3b26612b0/) (Materials and Content)\n",
    "- [Faith Yeo](https://www.linkedin.com/in/faithyjw/) (Publicity IC)\n",
    "\n",
    "\n",
    "The committee could not have done it without the close collaboration and support of **NYP AI committee members**, and student development executives ***Ms Teo Miow Ting*** and ***Mr Alvin Tay***.\n",
    "\n",
    "\n",
    "---\n",
    "**Inspired to join us** to create value for SIT students across several verticals in AI? [Join us](https://go.nyp.ai/join) or [visit our website](https://nyp.ai).\n",
    "\n",
    "\n",
    "We hope you had an enriching experience and we can't wait to see what you build.\n",
    "\n",
    "<strong>Signing off,<br>\n",
    "NYP Artificial Intelligence<br>\n",
    "NYP School of Information Technlogy</strong>\n",
    "\n",
    "<img src=\"https://about.nyp.ai/static/logo/Dark.png\" alt=\"NYP AI Logo\" height=\"100px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lYoAKa4d_RJK"
   },
   "source": [
    "*---- You have reached the end ----*\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
